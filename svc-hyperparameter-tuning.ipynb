{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10314246,"sourceType":"datasetVersion","datasetId":6239190}],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Hiperparameter tuning","metadata":{}},{"cell_type":"code","source":"# It is important to have the same version in Kaggle and in local\n# to save and import the models properly\n!pip install scikit-learn==1.5.2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T01:06:50.599520Z","iopub.execute_input":"2024-12-29T01:06:50.600125Z","iopub.status.idle":"2024-12-29T01:07:07.357119Z","shell.execute_reply.started":"2024-12-29T01:06:50.600067Z","shell.execute_reply":"2024-12-29T01:07:07.355756Z"}},"outputs":[{"name":"stdout","text":"Collecting scikit-learn==1.5.2\n  Downloading scikit_learn-1.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\nRequirement already satisfied: numpy>=1.19.5 in /opt/conda/lib/python3.10/site-packages (from scikit-learn==1.5.2) (1.26.4)\nRequirement already satisfied: scipy>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn==1.5.2) (1.14.1)\nRequirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn==1.5.2) (1.4.2)\nRequirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn==1.5.2) (3.5.0)\nDownloading scikit_learn-1.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m82.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: scikit-learn\n  Attempting uninstall: scikit-learn\n    Found existing installation: scikit-learn 1.2.2\n    Uninstalling scikit-learn-1.2.2:\n      Successfully uninstalled scikit-learn-1.2.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 0.22.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.10.0, but you have google-cloud-bigquery 2.34.4 which is incompatible.\nbigframes 0.22.0 requires google-cloud-storage>=2.0.0, but you have google-cloud-storage 1.44.0 which is incompatible.\nbigframes 0.22.0 requires pandas<2.1.4,>=1.5.0, but you have pandas 2.2.3 which is incompatible.\ncesium 0.12.3 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ndataproc-jupyter-plugin 0.1.79 requires pydantic~=1.10.0, but you have pydantic 2.10.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed scikit-learn-1.5.2\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport nltk\nfrom nltk.stem.wordnet import WordNetLemmatizer\nimport re\nimport string\nimport sklearn\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.metrics import accuracy_score, confusion_matrix, f1_score, precision_score, recall_score, make_scorer\nimport seaborn as sns\nimport matplotlib.pyplot as plt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T01:07:07.359671Z","iopub.execute_input":"2024-12-29T01:07:07.360007Z","iopub.status.idle":"2024-12-29T01:07:09.425957Z","shell.execute_reply.started":"2024-12-29T01:07:07.359974Z","shell.execute_reply":"2024-12-29T01:07:09.424985Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/webtexts-en/all-codes-small.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T01:07:09.427321Z","iopub.execute_input":"2024-12-29T01:07:09.427956Z","iopub.status.idle":"2024-12-29T01:07:11.252056Z","shell.execute_reply.started":"2024-12-29T01:07:09.427907Z","shell.execute_reply":"2024-12-29T01:07:11.250933Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"text_stream = [nltk.word_tokenize(d.lower()) for d in df.text.tolist()]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T01:07:11.254247Z","iopub.execute_input":"2024-12-29T01:07:11.254581Z","iopub.status.idle":"2024-12-29T01:08:07.991658Z","shell.execute_reply.started":"2024-12-29T01:07:11.254549Z","shell.execute_reply":"2024-12-29T01:08:07.990473Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"labels = df[\"label\"].tolist()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T01:08:07.993086Z","iopub.execute_input":"2024-12-29T01:08:07.993531Z","iopub.status.idle":"2024-12-29T01:08:07.999793Z","shell.execute_reply.started":"2024-12-29T01:08:07.993489Z","shell.execute_reply":"2024-12-29T01:08:07.998673Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"!unzip -n /usr/share/nltk_data/corpora/wordnet.zip -d /usr/share/nltk_data/corpora/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T01:08:08.001026Z","iopub.execute_input":"2024-12-29T01:08:08.001354Z","iopub.status.idle":"2024-12-29T01:08:09.572644Z","shell.execute_reply.started":"2024-12-29T01:08:08.001323Z","shell.execute_reply":"2024-12-29T01:08:09.571192Z"}},"outputs":[{"name":"stdout","text":"Archive:  /usr/share/nltk_data/corpora/wordnet.zip\n   creating: /usr/share/nltk_data/corpora/wordnet/\n  inflating: /usr/share/nltk_data/corpora/wordnet/lexnames  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.verb  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.adv  \n  inflating: /usr/share/nltk_data/corpora/wordnet/adv.exc  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.verb  \n  inflating: /usr/share/nltk_data/corpora/wordnet/cntlist.rev  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.adj  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.adj  \n  inflating: /usr/share/nltk_data/corpora/wordnet/LICENSE  \n  inflating: /usr/share/nltk_data/corpora/wordnet/citation.bib  \n  inflating: /usr/share/nltk_data/corpora/wordnet/noun.exc  \n  inflating: /usr/share/nltk_data/corpora/wordnet/verb.exc  \n  inflating: /usr/share/nltk_data/corpora/wordnet/README  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.sense  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.noun  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.adv  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.noun  \n  inflating: /usr/share/nltk_data/corpora/wordnet/adj.exc  \n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"def token_is_error_status_code(token):\n    try:\n        n = int(token)\n        return 400 <= n <= 425 or 500 <= n <= 511\n    except ValueError:\n        return False\n\ndef token_is_punctuation(token):\n    for c in token:\n        if c not in string.punctuation:\n            return False\n    return True\n\ndef is_token_just_word(token):\n    return re.match(\"^[a-z'_]+$\", token)\n\ndef clean_term_sentence(ts):\n    for token in ts:\n        token = token.strip()\n        token = token.replace(\"’\", \"'\")\n        if token and not token_is_punctuation(token) \\\n            and (is_token_just_word(token) or token_is_error_status_code(token)):\n            yield WordNetLemmatizer().lemmatize(token)\n\nclean_streams = [list(clean_term_sentence(ts)) for ts in text_stream]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T01:08:09.574954Z","iopub.execute_input":"2024-12-29T01:08:09.575508Z","iopub.status.idle":"2024-12-29T01:08:48.618792Z","shell.execute_reply.started":"2024-12-29T01:08:09.575452Z","shell.execute_reply":"2024-12-29T01:08:48.617364Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"documents = [\" \".join(ts) for ts in clean_streams]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T01:08:48.620464Z","iopub.execute_input":"2024-12-29T01:08:48.620892Z","iopub.status.idle":"2024-12-29T01:08:48.844034Z","shell.execute_reply.started":"2024-12-29T01:08:48.620859Z","shell.execute_reply":"2024-12-29T01:08:48.843007Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"my_vectorizer = TfidfVectorizer(\n    analyzer='word', \n    max_features=30000,\n    ngram_range=(1, 3)\n)\n\nX = my_vectorizer.fit_transform(documents)\n\ndoc_vectors = X.toarray()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T01:08:48.845470Z","iopub.execute_input":"2024-12-29T01:08:48.845959Z","iopub.status.idle":"2024-12-29T01:09:46.828058Z","shell.execute_reply.started":"2024-12-29T01:08:48.845921Z","shell.execute_reply":"2024-12-29T01:09:46.827028Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"X_train, X_test, y_train, y_test  = train_test_split(\n        doc_vectors, \n        labels,\n        train_size=0.80,\n        random_state=1234)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T01:09:46.831358Z","iopub.execute_input":"2024-12-29T01:09:46.832038Z","iopub.status.idle":"2024-12-29T01:09:52.949768Z","shell.execute_reply.started":"2024-12-29T01:09:46.831985Z","shell.execute_reply":"2024-12-29T01:09:52.948725Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"param_grid = {\n    # \"tol\": [1e-5, 1e-4, 1e-3], # Best = 1e-5\n    # 'C': [0.1, 1, 10, 100, 1000], # Best C = 1\n    # \"fit_intercept\": [True, False], # Best fit_intercept = True\n    # \"intercept_scaling\": [0.1, 1.0, 10], # Best = 0.1, but same as 1\n    # \"max_iter\": [1000, 2000, 3000],\n}\n\n# Create an SVM classifier\nsvm = LinearSVC()\n\n# Create a GridSearchCV object\ngrid_search = GridSearchCV(\n    estimator=svm,\n    param_grid=param_grid, \n    cv=5, \n    scoring=make_scorer(f1_score, pos_label=\"error\"),\n    verbose=10,\n)\n\n# Fit the GridSearchCV object to the training data\ngrid_search.fit(X_train, y_train)\n\n# Print the best hyperparameters and corresponding accuracy score\nprint(\"Best Hyperparameters: \", grid_search.best_params_)\nprint(\"Best Accuracy Score: {}\".format(grid_search.best_score_))\n\n# Evaluate the model on the test set\nbest_svm = grid_search.best_estimator_\ntest_accuracy = best_svm.score(X_test, y_test)\nprint(\"Test Accuracy: {}\".format(test_accuracy))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T01:20:45.093791Z","iopub.execute_input":"2024-12-29T01:20:45.094346Z","iopub.status.idle":"2024-12-29T01:23:31.162084Z","shell.execute_reply.started":"2024-12-29T01:20:45.094303Z","shell.execute_reply":"2024-12-29T01:23:31.159363Z"}},"outputs":[{"name":"stdout","text":"Fitting 5 folds for each of 3 candidates, totalling 15 fits\n[CV 1/5; 1/3] START intercept_scaling=0.1.......................................\n[CV 1/5; 1/3] END ........intercept_scaling=0.1;, score=0.980 total time=   7.3s\n[CV 2/5; 1/3] START intercept_scaling=0.1.......................................\n[CV 2/5; 1/3] END ........intercept_scaling=0.1;, score=0.982 total time=   7.1s\n[CV 3/5; 1/3] START intercept_scaling=0.1.......................................\n[CV 3/5; 1/3] END ........intercept_scaling=0.1;, score=0.981 total time=   7.0s\n[CV 4/5; 1/3] START intercept_scaling=0.1.......................................\n[CV 4/5; 1/3] END ........intercept_scaling=0.1;, score=0.983 total time=   7.0s\n[CV 5/5; 1/3] START intercept_scaling=0.1.......................................\n[CV 5/5; 1/3] END ........intercept_scaling=0.1;, score=0.984 total time=   7.1s\n[CV 1/5; 2/3] START intercept_scaling=1.0.......................................\n[CV 1/5; 2/3] END ........intercept_scaling=1.0;, score=0.980 total time=   7.2s\n[CV 2/5; 2/3] START intercept_scaling=1.0.......................................\n[CV 2/5; 2/3] END ........intercept_scaling=1.0;, score=0.982 total time=   7.1s\n[CV 3/5; 2/3] START intercept_scaling=1.0.......................................\n[CV 3/5; 2/3] END ........intercept_scaling=1.0;, score=0.981 total time=   7.1s\n[CV 4/5; 2/3] START intercept_scaling=1.0.......................................\n[CV 4/5; 2/3] END ........intercept_scaling=1.0;, score=0.983 total time=   7.2s\n[CV 5/5; 2/3] START intercept_scaling=1.0.......................................\n[CV 5/5; 2/3] END ........intercept_scaling=1.0;, score=0.984 total time=   7.2s\n[CV 1/5; 3/3] START intercept_scaling=10........................................\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"[CV 1/5; 3/3] END .........intercept_scaling=10;, score=0.980 total time=  15.7s\n[CV 2/5; 3/3] START intercept_scaling=10........................................\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"[CV 2/5; 3/3] END .........intercept_scaling=10;, score=0.982 total time=  15.6s\n[CV 3/5; 3/3] START intercept_scaling=10........................................\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"[CV 3/5; 3/3] END .........intercept_scaling=10;, score=0.981 total time=  15.6s\n[CV 4/5; 3/3] START intercept_scaling=10........................................\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"[CV 4/5; 3/3] END .........intercept_scaling=10;, score=0.983 total time=  15.8s\n[CV 5/5; 3/3] START intercept_scaling=10........................................\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"[CV 5/5; 3/3] END .........intercept_scaling=10;, score=0.984 total time=  15.8s\nBest Hyperparameters:  {'intercept_scaling': 0.1}\nBest Accuracy Score: 0.9820038009056908\nTest Accuracy: 0.9838474617439883\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}